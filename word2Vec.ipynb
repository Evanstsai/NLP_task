{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## referene: https://towardsdatascience.com/learn-word2vec-by-implementing-it-in-tensorflow-45641adaf2ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "corpus_raw = \"He is king. The king is loyal. She is the royal queen.\"\n",
    "\n",
    "#cover to lower case\n",
    "corpus_raw = corpus_raw.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Punctuations(string):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    no_pun = \"\"\n",
    "    for char in string:\n",
    "        if char not in punctuations:\n",
    "            no_pun += char\n",
    "    return(no_pun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "Remove_Punctuations = Remove_Punctuations(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "[words.append(word) for word in Remove_Punctuations.split() if word != '.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he',\n",
       " 'is',\n",
       " 'king',\n",
       " 'the',\n",
       " 'king',\n",
       " 'is',\n",
       " 'loyal',\n",
       " 'she',\n",
       " 'is',\n",
       " 'the',\n",
       " 'royal',\n",
       " 'queen']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datatype == set'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = set(words) ## remove duplicates   \n",
    "'''datatype == set'''\n",
    "#{'he', 'is', 'king', 'loyal', 'queen', 'royal', 'she', 'the'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2int = {}\n",
    "int2word = {}\n",
    "\n",
    "vocab_size = len(words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'the',\n",
       " 1: 'loyal',\n",
       " 2: 'royal',\n",
       " 3: 'she',\n",
       " 4: 'is',\n",
       " 5: 'he',\n",
       " 6: 'queen',\n",
       " 7: 'king'}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for i, word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "    int2word[i] = word\n",
    "\n",
    "int2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is king. the king is loyal. she is the royal queen.'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_sentence = corpus_raw.split(\".\")\n",
    "sentences = []\n",
    "[sentences.append(sentence.split()) for sentence in raw_sentense] #spilt(',') & split() different\n",
    "# for sentence in raw_sentence:\n",
    "#     sentences.append(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.remove(sentences[-1]) #remove last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['he', 'is', 'king'],\n",
       " ['the', 'king', 'is', 'loyal'],\n",
       " ['she', 'is', 'the', 'royal', 'queen']]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "window_size = 2\n",
    "for sentence in sentences:\n",
    "    for index, word in enumerate(sentence):\n",
    "        for nb_word in sentence[max(index - window_size, 0): min(index + window_size, len(sentence)) + 1] :\n",
    "            if nb_word != word:\n",
    "                data.append([word, nb_word])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['he', 'is'],\n",
       " ['he', 'king'],\n",
       " ['is', 'he'],\n",
       " ['is', 'king'],\n",
       " ['king', 'he'],\n",
       " ['king', 'is'],\n",
       " ['the', 'king'],\n",
       " ['the', 'is'],\n",
       " ['king', 'the'],\n",
       " ['king', 'is'],\n",
       " ['king', 'loyal'],\n",
       " ['is', 'the'],\n",
       " ['is', 'king'],\n",
       " ['is', 'loyal'],\n",
       " ['loyal', 'king'],\n",
       " ['loyal', 'is'],\n",
       " ['she', 'is'],\n",
       " ['she', 'the'],\n",
       " ['is', 'she'],\n",
       " ['is', 'the'],\n",
       " ['is', 'royal'],\n",
       " ['the', 'she'],\n",
       " ['the', 'is'],\n",
       " ['the', 'royal'],\n",
       " ['the', 'queen'],\n",
       " ['royal', 'is'],\n",
       " ['royal', 'the'],\n",
       " ['royal', 'queen'],\n",
       " ['queen', 'the'],\n",
       " ['queen', 'royal']]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to one-hot \n",
    "\n",
    "def to_one_hot(data_index, vocab_size):\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[data_index] = 1\n",
    "    return temp\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for data_word in data:\n",
    "    X_train.append(to_one_hot(word2int[data_word[0]],vocab_size))\n",
    "    y_train.append(to_one_hot(word2int[data_word[1]],vocab_size))\n",
    "    \n",
    "#to np.array\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 8) (30, 8)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make TF model\n",
    "x = tf.placeholder(tf.float32, shape = (None, vocab_size))\n",
    "y_label = tf.placeholder(tf.float32, shape = (None, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 5\n",
    "w1 = tf.Variable(tf.random_normal(shape = [vocab_size, embedding_dim]))\n",
    "b1 = tf.Variable(tf.random_normal([embedding_dim])) #if no [], it would erroe\n",
    "\n",
    "h1 = tf.add(tf.matmul(x,w1), b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deconvolution\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal([embedding_dim, vocab_size]))\n",
    "b2 = tf.Variable(tf.random_normal([vocab_size]))\n",
    "\n",
    "prediction = tf.nn.softmax(tf.add(tf.matmul(h1, w2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "501 % 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter : 0 ,loss is :5.454780101776123\n",
      "iter : 500 ,loss is :3.1797587871551514\n",
      "iter : 1000 ,loss is :2.727567434310913\n",
      "iter : 1500 ,loss is :2.5255672931671143\n",
      "iter : 2000 ,loss is :2.3933401107788086\n",
      "iter : 2500 ,loss is :2.294651746749878\n",
      "iter : 3000 ,loss is :2.215737819671631\n",
      "iter : 3500 ,loss is :2.1498067378997803\n",
      "iter : 4000 ,loss is :2.0931475162506104\n",
      "iter : 4500 ,loss is :2.043578624725342\n",
      "iter : 5000 ,loss is :1.9997187852859497\n",
      "iter : 5500 ,loss is :1.9606295824050903\n",
      "iter : 6000 ,loss is :1.925628423690796\n",
      "iter : 6500 ,loss is :1.8941948413848877\n",
      "iter : 7000 ,loss is :1.8659082651138306\n",
      "iter : 7500 ,loss is :1.8404203653335571\n",
      "iter : 8000 ,loss is :1.8174313306808472\n",
      "iter : 8500 ,loss is :1.7966763973236084\n",
      "iter : 9000 ,loss is :1.777917504310608\n",
      "iter : 9500 ,loss is :1.760936975479126\n",
      "iter : 10000 ,loss is :1.7455357313156128\n"
     ]
    }
   ],
   "source": [
    "## training\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "#define loss function\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), axis = 1))\n",
    "\n",
    "#define training steps\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(cross_entropy_loss)\n",
    "\n",
    "n_iter = 10001\n",
    "for i in range(n_iter):\n",
    "    sess.run(train_step, feed_dict = {x:X_train, y_label: y_train})\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        print(\"iter : {} ,loss is :{}\".format( i ,sess.run(cross_entropy_loss, feed_dict = {x:X_train, y_label : y_train})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.71522689  0.82355374  0.50510323  0.23424508  0.15557805]\n",
      " [-0.90964729 -0.4313322   0.14567941 -0.17226715 -0.31882137]\n",
      " [-0.79230016  0.48592439 -0.62649763 -0.01178129 -1.71997523]\n",
      " [ 1.57645917  0.60581106 -2.48719311 -1.01684153 -0.66595626]\n",
      " [-0.34832969 -0.22489746  0.10805894  0.26057795 -0.1220635 ]\n",
      " [-1.60010123 -0.2585701  -0.45608088 -0.24865095 -0.29532483]\n",
      " [-0.34532425  0.6934796  -0.6759395  -0.27927086 -0.69181055]\n",
      " [-0.09686808  0.22571857 -0.5894956  -0.59333175 -0.95991462]]\n",
      "----------\n",
      "[-0.41202977  0.90769064 -0.27769288 -1.18541813  0.02963074]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(w1))\n",
    "print('----------')\n",
    "print(sess.run(b1))\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform vector\n",
    "vectors = sess.run(w1 + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.75735402  1.6011703  -0.95363235 -1.46468902 -0.66217983]\n"
     ]
    }
   ],
   "source": [
    "print(vectors[ word2int['queen'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here’s a quick function to find the closest vector to a given vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(vec1, vec2):\n",
    "    return np.sqrt(np.sum((vec1-vec2)**2))\n",
    "\n",
    "def find_closest(word_index, vectors):\n",
    "    min_dist = 10000 # to act like positive infinity\n",
    "    min_index = -1\n",
    "    query_vector = vectors[word_index]\n",
    "    for index, vector in enumerate(vectors):\n",
    "        if euclidean_dist(vector, query_vector) < min_dist and not np.array_equal(vector, query_vector):\n",
    "            min_dist = euclidean_dist(vector, query_vector)\n",
    "            min_index = index\n",
    "    return min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen\n",
      "king\n",
      "queen\n"
     ]
    }
   ],
   "source": [
    "print(int2word[find_closest(word2int['king'], vectors)])\n",
    "print(int2word[find_closest(word2int['queen'], vectors)])\n",
    "print(int2word[find_closest(word2int['royal'], vectors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE tSNE\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "np.set_printoptions(suppress=True)\n",
    "vectors = model.fit_transform(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "normalizer = preprocessing.Normalizer()\n",
    "vectors =  normalizer.fit_transform(vectors, 'l2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the -0.994939\n",
      "loyal 0.916676\n",
      "royal 0.375739\n",
      "she 0.373768\n",
      "is 0.944338\n",
      "he -0.55404\n",
      "queen 0.937099\n",
      "king -0.451298\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe8391af908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "for word in words:\n",
    "    print(word, vectors[word2int[word]][1])\n",
    "    ax.annotate(word, (vectors[word2int[word]][0],vectors[word2int[word]][1] ))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
